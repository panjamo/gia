//! Ollama provider implementation for local LLM integration.
//!
//! # Token Usage Limitation
//!
//! The `genai` crate (v0.4) does not currently expose token usage statistics
//! for Ollama responses. All token counts default to zero. This is a known
//! limitation of the underlying library, not this implementation.

use crate::conversation::TokenUsage;
use crate::logging::{log_debug, log_info};
use crate::provider::{AiProvider, AiResponse};
use anyhow::{Context, Result};
use async_trait::async_trait;
use genai::Client;
use genai::chat::{ChatMessage, ChatRequest, ChatResponse};

#[derive(Debug)]
pub struct OllamaClient {
    model: String,
    client: Client,
}

impl OllamaClient {
    pub fn new(model: String) -> Result<Self> {
        log_info(&format!("Initializing Ollama client with model: {}", model));

        let client = Client::default();

        Ok(Self { model, client })
    }
}

#[async_trait]
impl AiProvider for OllamaClient {
    async fn generate_content_with_chat_messages(
        &mut self,
        chat_messages: Vec<ChatMessage>,
    ) -> Result<AiResponse> {
        log_debug(&format!(
            "Sending chat request to Ollama API with {} message(s)",
            chat_messages.len()
        ));

        let chat_req = ChatRequest::new(chat_messages);

        let chat_res = self
            .client
            .exec_chat(&self.model, chat_req, None)
            .await
            .context("Failed to execute Ollama chat request")?;

        let content = chat_res.first_text().unwrap_or("").to_string();

        if content.trim().is_empty() {
            return Err(anyhow::anyhow!(
                "No content was generated by the AI. The response was empty or contained only whitespace."
            ));
        }

        log_info(&format!(
            "Received response from Ollama API, length: {}",
            content.len()
        ));

        // genai doesn't expose token usage for Ollama, use default
        let usage = TokenUsage::default();

        Ok(AiResponse { content, usage })
    }

    async fn generate_content_with_request(
        &mut self,
        chat_request: ChatRequest,
    ) -> Result<ChatResponse> {
        let client = Client::default();
        let response = client
            .exec_chat(&self.model, chat_request, None)
            .await
            .context("Failed to send chat request to Ollama")?;
        Ok(response)
    }

    fn model_name(&self) -> &str {
        &self.model
    }

    fn provider_name(&self) -> &'static str {
        "Ollama"
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use genai::chat::{ChatMessage, MessageContent};

    #[tokio::test]
    async fn test_ollama_client_creation() {
        let model = "llama3.2".to_string();
        let client = OllamaClient::new(model.clone());
        assert!(client.is_ok());

        let client = client.unwrap();
        assert_eq!(client.model_name(), &model);
        assert_eq!(client.provider_name(), "Ollama");
    }

    #[tokio::test]
    async fn test_ollama_error_handling() {
        let mut client = OllamaClient::new("llama3.2".to_string()).unwrap();

        let messages = vec![ChatMessage {
            role: genai::chat::ChatRole::User,
            content: MessageContent::from_text(""),
            options: None,
        }];

        // Will fail with connection error or empty response error
        let result = client.generate_content_with_chat_messages(messages).await;
        assert!(result.is_err());
    }
}
